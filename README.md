# mini-transformer-from-scratch
Tiny, well-commented Transformer **encoder** in PyTorch: - Scaled dot-product attention - Multi-head attention - Add &amp; Norm, Feed-Forward, Positional Encoding - Trains on a toy **masked-language** task over synthetic tokens
